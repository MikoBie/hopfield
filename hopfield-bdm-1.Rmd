---
title: "Preliminary analysis of algorithmic complexity of Hopfield model"
description: |
    The main goal is to differentiate between sub-, super- and critical states of Hopfield model
    based solely on metrics derived from algorithmic complexity of the system.
author:
  - name: Szymon Talaga and Miko≈Çaj Biesaga
    affiliation: The Robert Zajonc Institute for Social Studies
    affiliation_url: http://iss.uw.edu.pl/en/
date: "`r Sys.Date()`"
output: radix::radix_article
bibliography: Algorithmic-Information.bib
---

## Introduction

In this report we present preliminary results assessing efficacy of algorithmic complexity (AC) as a measure
for differentiating between different regimes in Hopfield model. In all cases the AC approach is contrasted 
with standard block entropy in order to determine the extent to which it provides advantage over more 
traditional methods. More concretely, the goal is to differentiate between Hopfield networks in critical,
subcritical and supercritical phases based on their dynamics with respect to the attractor representing
memorized state. Here, we analyze only $6$ histories of dynamics --- $2$ for each type of phase.
Each Hopfield networks consists of $196$ nodes and is evolved for $1000$ time steps.
We analyze the networks' dynamics with respect to two fundamental features:

1. Magnetization, which is the fraction of states that are congruent with the attractor state.
   For each network there is a time series of $1000$ recordings of magnetization.
2. Congruence vectors, which are binary vectors indicating nodes that agree with the attractor.
   For each networks there is a time series of $1000$ congruent vectors of length $196$.

Before presenting the results, we briefly introduce main ideas from algorithmic information theory on which
our approach is based on.

### Algorithmic information theory

The main ideas we present here are discussed in details in [@soler-toscano_calculating_2014]
(general algorithmic information theory and Coding Theorem Method)
and [@zenil_decomposition_2018] (Block Decomposition Method). The presentation of the theory we give here is
largely simplified for the sake of brevity.

Algorithmic complexity (called also Kolmogorov / Kolmogorov-Chaitin complexity) corresponds to the length
of the shortest computer program that generates a given data set. More formally it can be stated as:

$$
K_T(s) = \min\{|p|, T(p) = s\}
$$
where $s$ is a string $p$ is a program, $|p|$ is its length in bits, $T$ is a universal Turing machine 
and $T(p)$ is the output of $p$ run on $T$ 
(here we consider binary strings only, but all results can be generalized at least to two-dimensional binary data sets
such as adjacency matrices).
As such algorithmic complexity is a proper mathematical measure
of compression and as a result true randomness. It is so because some patterns may look randomly from the
vantage point of probability theory, but still be generated by very simple set of rules. For instance, decimal
expansion of $\pi$ is said to be Borel-normal --- asymptotically every possible subsequence of every length
occurs there with equal probability --- but we know that it can be generated with relatively simple formulas.

The problem with algorithmic complexity is the fact that it is not computable in the general case. This stems from
the fact that to compute it one has to search through the space of all possible Turing machines. This is not only
extremely expensive in terms of computation time as the space is very vast, but simply impossible due to the so-called
halting problems --- there are Turing machines that never stop to run and in general there is no way to determine
whether a given machine will ever stop. Therefore algorithmic complexity can only be approximated.

The state-of-the-art approach to approximating algorithmic complexity is based on the notion of algorithmic probability,
which corresponds to the expected probability that a random program $p$ running on a universal prefix-free Turing
machine $T$ will output the string $s$. Formally it can be defined as:

$$
m(s) = \sum_{p: T(p) = s} 1 / 2^{|p|}
$$
This is an important quantity, because by the Coding Theorem we know that there is a constant $c$ such that:

$$
|-\log_2m(s) - K(s)| < c
$$
And by rearranging we get:

$$
K(s) = -\log_2m(s) + O(1)
$$
Hence, the problem now reduces to approximation of the algorithmic probability of a string.
Putting aside some technical details related to problem of Turing machines that never halts $m(s)$ can be approximated
as follows:

$$
m(s) \approx \frac{|\{T : T(p) = s\}|}{|\{T : T \text{ halts}\}|}
$$
In other words it can be approximated simply as a fraction of Turing machines yielding $s$ among all Turing machines
that ever halt. This approach to approximating AC is called *Coding Theorem Method* (CTM).

### Block Decomposition Method

Now, the problem of approximating algorithmic complexity is theoretically solved. 
However, the solution proposed so far is of no practical use, as every such computation would require and an incredible
amount of computing time due to the fact that every time a vast space of possible Turing machines has to be searched.
Fortunately, this problem can be solved with the so-called *Block Decomposition Method* (BDM).

The idea is simple. First we precompute CTM values for all possible **small** data set of a given kind
(i.e. every possible binary string of length up to $12$ of every possible square binary matrix of size up to $4$-by-$4$)
and store them in a look-up table. Then, any data set of arbitrary size can be decomposed into small blocks 
of appropriate size and algorithmic complexity of each block can be efficiently read from the look-up table of CTM values.
Last but not least, there is a theoretically informed way to aggregate CTM values for blocks into a final complexity
approximation for the entire data set. It can be defined as follows:

$$
\text{BDM}(s) = \sum_i \text{CTM}(s_i) + \log_2(n_i)
$$
where $s_i$ are all unique blocks a data set $s$ is decomposed to, $\text{CTM}(s_i)$ are CTM values of unique blocks
and $n_i$ are the numbers of occurrences of unique blocks. In the next section of this report we use exactly this
approach to approximate algorithmic complexity of Hopfield networks. Also, by entropy we will mean block entropy
for blocks decomposed in the exactly same way.

It is also important to note that both BDM and entropy values can be normalized by considering values for the simplest
possible data sets (constant, i.e. only $0$ or $1$) and maximally complex data sets composed of blocks with decreasing
CTM values. To facilitate comparisons between BDM and entropy we always use normalized values.

## Results

Now we present the results of the analysis of complexity of Hopfield model. We consider the following measure:

1. **Magnetization.** To apply BDM magnetization values have to be first discretized. We use intervals of equal length
   and cut into $2$, $4$, $5$, $6$ and $9$ pieces (this is limited by the available precomputed CTM data sets).
   Then, complexity values correspond to how randomly a time series walks through different intervals of magnetization.
   Note, that in this case networks with highly volatile magnetization should have high complexity.

```{r setup_knitr, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r setup_environment}
library(tidyverse)
library(reticulate)
library(ggpubr)
library(nlme)
library(emmeans)
library(broom)

# Set up ggplot aesthetics
theme_set(theme_bw())
# Use conda env for python
use_condaenv("bdm")

# Globals
DATA_PATH <- normalizePath(file.path( '.','..', 'data', 'base-model'))
COLORS <- c("green3", "red3", "cornflowerblue")
```

```{r functions}
# Discretize to uniform bins
discretize_u <- function(x, n) {
    cut(x, breaks = 0:n/n, include.lowest = TRUE)%>%
        as.integer
}
```

```{r get_data}
# Get magnetization data
idx <- 0L
M <- map(dir(file.path(DATA_PATH, "Mags"), pattern = "\\.csv$", full.names = TRUE), ~{
    phase <- case_when(
        str_detect(.x, regex("subCritical", ignore_case = FALSE)) ~ "sub",
        str_detect(.x, regex("superCritical", ignore_case = FALSE)) ~ "super",
        TRUE ~ "critical"
    )
    idx <<- idx + 1L
    df <- read_tsv(.x, col_names = c("t", "m"), col_types = "id")
    df <- df %>% add_column(
        idx = idx,
        phase = phase,
        .before = 1L
    )
    df
}) %>%
    bind_rows %>%
    group_by(phase) %>%
    mutate(idx = if_else(idx == min(idx), 1, 2)) %>%
    # ungroup %>%
    mutate(
        # discretize to uniform bins
        m_u2 = discretize_u(m, 2),
        m_u4 = discretize_u(m, 4),
        m_u5 = discretize_u(m, 5),
        m_u6 = discretize_u(m, 6),
        m_u9 = discretize_u(m, 9),
    ) %>%
    ungroup %>%
    mutate(phase = factor(phase, levels = c("sub", "critical", "super")))
```

```{r viz_magnetization_density, fig.width = 6, fig.asp = .618}
M %>%
    ggplot(aes(x = m, fill = as.factor(idx))) +
    geom_density() +
    facet_wrap(~phase, ncol = 1, scales = "free_y") +
    scale_fill_manual(values = COLORS, name = "") +
    labs(x = "Magnetization", y = "Probability density")
```

```{r viz_magnetization_ts, fig.width = 6, fig.asp = .618}
M %>%
    ggplot(aes(x = t, y = m, color = as.factor(idx)), group = as.factor(idx)) +
    geom_line() +
    facet_wrap(~phase, ncol = 1) +
    scale_color_manual(values = COLORS, name = "") +
    labs(x = "Time step", y = "Magnetization")
```

```{python get_bdm}
import warnings
import numpy as np
from bdm import BDM

def get_bdm(x, nsymbols, ent=False, shape=12):
    shape=int(shape)
    x = np.array([ _ for _ in x ]).astype(int)
    bdm = BDM(ndim=1, nsymbols=nsymbols, shape=(shape,))#, warn_if_missing_ctm=False)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore')
        return bdm.ent(x) if ent else bdm.bdm(x)
```

```{r compute_bdm, warning=FALSE}
# Complexity measures for magnetization
C_m <- map(c(2, 4, 5, 6, 9), ~{
    c6 <- sym(str_c("cmx6_u", .x))
    e6 <- sym(str_c("ent6_u", .x))
    c7 <- sym(str_c("cmx7_u", .x))
    e7 <- sym(str_c("ent7_u", .x))
    c8 <- sym(str_c("cmx8_u", .x))
    e8 <- sym(str_c("ent8_u", .x))
    c9 <- sym(str_c("cmx9_u", .x))
    e9 <- sym(str_c("ent9_u", .x))
    c10 <- sym(str_c("cmx10_u", .x))
    e10 <- sym(str_c("ent10_u", .x))
    c11 <- sym(str_c("cmx11_u", .x))
    e11 <- sym(str_c("ent11_u", .x))
    c12 <- sym(str_c("cmx12_u", .x))
    e12 <- sym(str_c("ent12_u", .x))
    m1 <- sym(str_c("m_u", .x))
    gdf <- M %>%
        group_by(phase, idx) %>%
        summarize(
            !!c6 := py$get_bdm(!!m1, nsymbols = .x, shape=6),
            !!e6 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=6),
            !!c7 := py$get_bdm(!!m1, nsymbols = .x, shape=7),
            !!e7 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=7),
            !!c8 := py$get_bdm(!!m1, nsymbols = .x, shape=8),
            !!e8 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=8),
            !!c9 := py$get_bdm(!!m1, nsymbols = .x, shape=9),
            !!e9 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=9),
            !!c10 := py$get_bdm(!!m1, nsymbols = .x, shape=10),
            !!e10 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=10),
            !!c11 := py$get_bdm(!!m1, nsymbols = .x, shape=11),
            !!e11 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=11),
            !!c12 := py$get_bdm(!!m1, nsymbols = .x, shape=12),
            !!e12 := py$get_bdm(!!m1, nsymbols = .x, ent = TRUE, shape=12),
        ) %>%
        ungroup
}) %>%
    reduce(left_join, by = c("idx", "phase")) %>%
    select(idx, phase, matches("cmx\\d+_u"), matches("ent\\d+_u"), matches("cmx\\d+_q"), matches("ent\\d+_q"))
```

### Magnetization

```{r magnetization_ts, fig.width = 6, fig.asp = .618}
C_m %>%
    select(-matches("[dq]\\d")) %>%
    gather(key = "metric", value = "value", -phase, -idx) %>%
    mutate(
        measure = if_else(str_detect(metric, "^cmx"), "algorithmic complexity", "block entropy"),
        nsymbols = as.integer(str_extract(metric, "\\d$")),
        block_size = str_replace(metric,"^\\D*(\\d+).*$","\\1") %>% as.integer()
    ) %>%
    group_by(idx, phase, measure, nsymbols) %>%
    summarise(mean = mean(value),
              n = n(),
              sd = sd(value)) %>%
    mutate(se = sd/sqrt(n)) %>%
    ggplot(aes(x = nsymbols, y = mean, color = phase, group = interaction(phase, idx))) +
    geom_line() +
    geom_point(pch = 16, size = 3) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.7,position=position_dodge(0.05)) +
    facet_wrap(~measure, scales = "free_y") +
    scale_color_manual(values = COLORS, name = "Phase") +
    scale_x_continuous(breaks = c(2, 4, 5, 6, 9)) +
    labs(x = "Number of equal length bins", y = "") +
    theme(legend.position = "top")
```
