---
title: "Hopfield network bdm"
description: |
    First attempt to compute bdm on Hopfield networks simulation data.
author:
  - name: Szymon Talaga and Miko≈Çaj Biesaga
    affiliation: The Robert Zajonc Institute for Social Studies
    affiliation_url: www.iss.uw.edu.pl/en/
date: "`r Sys.Date()`"
output: radix::radix_article
---

```{r setup_env, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 8, fig.asp = 1)
```

```{r setup}
library(tidyverse)
library(readr)
library(stringr)
library(purrr)
library(reticulate)
library(magrittr)
library(zoo)
library(boot)
use_condaenv("bdm")
theme_set(theme_bw())
```

## Magnetization

```{r read_data_magnetization}
path_mags <- 'data/base-model/Mags/'
list_of_files <- list.files(path = path_mags, pattern = '.csv')

mags <- list()
for (file_name in list_of_files) {
  name <- file_name %>%
    word(start = 2,
         end = 3,
         sep = "_") %>%
    str_extract(pattern = '\\w+_\\d')
  mags[[name]] <- read_delim(file = paste0(path_mags,file_name),
                          delim = '\t',
                          escape_double = FALSE,
                          col_names = FALSE,
                          trim_ws = TRUE) %>%
    rename(timestep = X1,
           magnetization = X2) %>%
    mutate(name = name)
}
mags_binded <- mags %>%
  bind_rows()
```

```{r mags_histograms}
mags_binded %>%
  group_by(name) %>%
  ggplot(aes(x = magnetization)) +
  geom_histogram() +
  facet_wrap(~name)
```

There is no point of doing anything with magnetization of subCritical because it is almost always 1. The only difference between subCritical_1 and subCritical_2 is that the former reaches 1 after 5 steps, and the latter after 13 steps.

To compute algorithmic complexity on magnetization data we first had to bin the data. We figured that there are two reasonable ways of doing it: compute quantiles or compute equally long bins.  The current version of Szymon's package limits computing algorithmic complexity to series with maximum 9 different elements. Therefore, we computed bins only up to this number of elements.

### Quantiles

```{r quantiles}
mags_quantiles <- mags[c(1,2,5,6)] %>%
  map_depth(1, function(table){
    table %>%
      mutate("quantile9" = cut(magnetization,
                            breaks = quantile(magnetization,
                                              probs = seq(0, 1, 1/9)),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0','1','2','3','4','5','6','7','8')),
             "quantile2" = cut(magnetization,
                            breaks = quantile(magnetization,
                                              probs = seq(0, 1, 1/2)),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0', '1')),
             "quantile4" = cut(magnetization,
                            breaks = quantile(magnetization,
                                              probs = seq(0, 1, 1/4)),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0', '1', '2', '3')),
             "quantile5" = cut(magnetization,
                            breaks = quantile(magnetization,
                                              probs = seq(0, 1, 1/5)),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0','1','2','3','4')),
             "quantile6" = cut(magnetization,
                            breaks = quantile(magnetization,
                                              probs = seq(0, 1, 1/6)),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0','1','2','3','4','5')))
  }) %>%
  bind_rows() %>%
  group_by(name) %>%
  summarise(seq9 = list(quantile9),
            seq2 = list(quantile2),
            seq4 = list(quantile4),
            seq5 = list(quantile5),
            seq6 = list(quantile6))
```

```{python quantiles_cmx}
import numpy as np
import pandas as pd
from bdm import BDMIgnore as BDM

data = r.mags_quantiles
bdm9 = BDM(ndim=1, nsymbols=9)
bdm2 = BDM(ndim=1, nsymbols=2)
bdm4 = BDM(ndim=1, nsymbols=4)
bdm5 = BDM(ndim=1, nsymbols=5)
bdm6 = BDM(ndim=1, nsymbols=6)

data.seq9 = data.seq9.apply(lambda x: np.array(x, dtype = int))
data.seq2 = data.seq2.apply(lambda x: np.array(x, dtype = int))
data.seq4 = data.seq4.apply(lambda x: np.array(x, dtype = int))
data.seq5 = data.seq5.apply(lambda x: np.array(x, dtype = int))
data.seq6 = data.seq6.apply(lambda x: np.array(x, dtype = int))

mags_quantiles = pd.DataFrame({"name": data.name,
                               "ncmx9": data.seq9.apply(bdm9.nbdm),
                               "cmx9": data.seq9.apply(bdm9.bdm),
                               "ncmx2": data.seq2.apply(bdm2.nbdm),
                               "cmx2": data.seq2.apply(bdm2.bdm),
                               "ncmx4": data.seq4.apply(bdm4.nbdm),
                               "cmx4": data.seq4.apply(bdm4.bdm),
                               "ncmx5": data.seq5.apply(bdm5.nbdm),
                               "cmx5": data.seq5.apply(bdm5.bdm),
                               "ncmx6": data.seq6.apply(bdm6.nbdm),
                               "cmx6": data.seq6.apply(bdm6.bdm)})

```



### Equal bins

```{r even_bins}
mags_equal_bins <- mags[c(1,2,5,6)] %>%
  map_depth(1, function(table){
    table %>%
      mutate("bins9" = cut(magnetization,
                            breaks = seq(min(magnetization),
                                         max(magnetization),
                                         (max(magnetization) - min(magnetization))/9),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0','1','2','3','4','5','6','7','8')),
             "bins2" = cut(magnetization,
                            breaks = seq(min(magnetization),
                                         max(magnetization),
                                         (max(magnetization) - min(magnetization))/2),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0', '1')),
             "bins4" = cut(magnetization,
                            breaks = seq(min(magnetization),
                                         max(magnetization),
                                         (max(magnetization) - min(magnetization))/4),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0', '1', '2', '3')),
             "bins5" = cut(magnetization,
                            breaks = seq(min(magnetization),
                                         max(magnetization),
                                         (max(magnetization) - min(magnetization))/5),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0','1','2','3','4')),
             "bins6" = cut(magnetization,
                            breaks = seq(min(magnetization),
                                         max(magnetization),
                                         (max(magnetization) - min(magnetization))/6),
                            include.lowest = TRUE,
                            right = TRUE,
                            labels = c('0','1','2','3','4','5')))
  }) %>%
  bind_rows() %>%
  group_by(name) %>%
  summarise(seq9 = list(bins9),
            seq2 = list(bins2),
            seq4 = list(bins4),
            seq5 = list(bins5),
            seq6 = list(bins6))
```

```{python even_bins_cmx}
import numpy as np
import pandas as pd
from bdm import BDMIgnore as BDM

data = r.mags_equal_bins
bdm9 = BDM(ndim=1, nsymbols=9)
bdm2 = BDM(ndim=1, nsymbols=2)
bdm5 = BDM(ndim=1, nsymbols=5)
bdm6 = BDM(ndim=1, nsymbols=6)

data.seq9 = data.seq9.apply(lambda x: np.array(x, dtype = int))
data.seq2 = data.seq2.apply(lambda x: np.array(x, dtype = int))
data.seq4 = data.seq4.apply(lambda x: np.array(x, dtype = int))
data.seq5 = data.seq5.apply(lambda x: np.array(x, dtype = int))
data.seq6 = data.seq6.apply(lambda x: np.array(x, dtype = int))

mags_equal_bins = pd.DataFrame({"name": data.name,
                               "ncmx9": data.seq9.apply(bdm9.nbdm),
                               "cmx9": data.seq9.apply(bdm9.bdm),
                               "ncmx2": data.seq2.apply(bdm2.nbdm),
                               "cmx2": data.seq2.apply(bdm2.bdm),
                               "ncmx4": data.seq4.apply(bdm4.nbdm),
                               "cmx4": data.seq4.apply(bdm4.bdm),
                               "ncmx5": data.seq5.apply(bdm5.nbdm),
                               "cmx5": data.seq5.apply(bdm5.bdm),
                               "ncmx6": data.seq6.apply(bdm6.nbdm),
                               "cmx6": data.seq6.apply(bdm6.bdm)})
```

### The difference between quantiles and equal bins

```{r difference_between_bins}
mags_quantiles_cmx <- py$mags_quantiles %>%
  mutate(bins = "quantiles") %>%
  unnest()
mags_equal_bins_cmx <- py$mags_equal_bins %>%
  mutate(bins = "equal_bins") %>%
  unnest()



mags_quantiles_cmx %>%
  bind_rows(mags_equal_bins_cmx) %>%
  gather(key = "klucz",
         value = "wartosc",
         ncmx9:cmx6) %>%
  mutate(klucz = paste0(klucz,"_", name)) %>%
  select(-name) %>%
  filter(!grepl(klucz, pattern = "^ncmx\\d")) %>%
  spread(bins, wartosc) %>%
  mutate(difference = even_bins - quantiles)
  
```

## States

```{r read_data_states}
path_states <- 'data/base-model/States/'
list_of_files <- list.files(path = path_states, pattern = '.csv')

states <- list()
for (file_name in list_of_files) {
  name <- file_name %>%
    word(start = 2,
         end = 3,
         sep = "_") %>%
    str_extract(pattern = '\\w+_\\d')
  states[[name]] <- read_delim(file = paste0(path_states,file_name),
                          delim = '\t',
                          escape_double = FALSE,
                          col_names = FALSE,
                          trim_ws = TRUE) %>%
    rename(timestep = X1,
           instantaneous = X2,
           preferred = X3) %>%
    group_by(timestep) %>%
    mutate(id = 1:n())
}
```

```{r prepare_data_for_bdm}
states_bdm <- map_depth(states, 1, function(table){
  table %>%
    mutate(compability = instantaneous * preferred) %>%
    gather(key = 'klucz',
           value = 'bit',
           c(instantaneous, compability, preferred)) %>%
    mutate(bit = (bit + 1)/2) %>%
    group_by(timestep, klucz) %>%
    summarise(seq = list(bit))
})

```

First, we computed a measure of compatibility between preferred and instantaneous states. This way we were able to see whether the preferred state matched the instantaneous state. We did it by just multiplying these two states by each other ($compability = instantaneous \times preferred$). Afterward, we transformed the data from the two-element alphabet of -1 and 1 into two-element alphabet of 0 and 1.

Apart from computing the change over time of the algorithmic complexity we also calculated series entropy. We wanted to see whether the algorithmic complexity is a more sensitive measure which allows distinguishing smaller differences between series. We used normalized algorithmic complexity and normalized entropy. Therefore, both measures have limits in 0 and 1.

```{python compute_states_cmx}
import numpy as np
import pandas as pd
from bdm import BDMIgnore as BDM

bdm = BDM(ndim=1)
    
data = pd.DataFrame.from_dict(r.states_bdm, orient = 'index').rename(columns = {0: 'frames'})
result = []

for i in range(6):
   data.frames[i].seq = pd.Series(data.frames[i].seq.apply(lambda x: np.array(x, dtype = int)))
   result.append(pd.DataFrame({'timestep': data.frames[i].timestep.astype(int),
                               'cmx': data.frames[i].seq.apply(bdm.bdm),
                               'ncmx': data.frames[i].seq.apply(bdm.nbdm),
                               'ent': data.frames[i].seq.apply(bdm.ent),
                               'nent': data.frames[i].seq.apply(bdm.nent),
                               'type': data.frames[i].klucz,
                               'name': data.index[i]}))

```

```{r prepare_states_for_viz}
states_cmx <- py$result %>%
  bind_rows()
```

### Time series of Algorithmic Complexity

```{r chart_cmx}
states_cmx %>%
  filter(type != 'preferred') %>%
  group_by(type, name) %>%
  mutate(rmean_ncmx = rollmean(ncmx, k = 7, align = "center", na.pad = TRUE),
         cumsum_ncmx = cumsum(ncmx)) %>%
  ggplot(aes(x = timestep, y = ncmx, color = name)) +
  facet_wrap(~type) +
  geom_line() +
  geom_smooth(method = "loess", linetype = 'dashed', se = FALSE, alpha = .5) +
  labs(y = "Normalized Algorithmic Complexity",
       x = "Time step") +
  scale_color_discrete("")

states_cmx %>%
  filter(type != 'preferred') %>%
  group_by(type, name) %>%
  mutate(rmean_ncmx = rollmean(ncmx, k = 7, align = "center", na.pad = TRUE),
         cumsum_ncmx = cumsum(ncmx)) %>%
  ggplot(aes(x = timestep, y = cumsum_ncmx, color = name)) +
  facet_wrap(~type) +
  geom_line() +
  labs(y = "Cumulative Sum of Normalized Algorithmic Complexity",
       x = "Time step") +
  scale_color_discrete("")
```

### Time series of Entropy

```{r chart_ent}
states_cmx %>%
  filter(type != 'preferred') %>%
  group_by(type,name) %>%
  mutate(rmean_nent = rollmean(nent, k = 7, align = "center", na.pad = TRUE),
         cumsum_nent = cumsum(nent)) %>%
  ggplot(aes(x = timestep, y = nent, color = name)) +
  facet_wrap(~type) +
  geom_line() +
  geom_smooth(method = "loess", linetype = 'dashed', se = FALSE, alpha = .5) +
  labs(y = "Normalized Entropy",
       x = "Time step") +
  scale_color_discrete("")

states_cmx %>%
  filter(type != 'preferred') %>%
  group_by(type,name) %>%
  mutate(rmean_nent = rollmean(nent, k = 7, align = "center", na.pad = TRUE),
         cumsum_nent = cumsum(nent)) %>%
  ggplot(aes(x = timestep, y = cumsum_nent, color = name)) +
  facet_wrap(~type) +
  geom_line() +
  labs(y = "Cumulative Sum of Normalized Entropy",
       x = "Time step") +
  scale_color_discrete("")
```

### Bootstrap non-parametric variance and mean test

We wanted to test the hypothesis that the algorithmic complexity is a more sensitive measure than entropy. We focused here only on comparison between critical_2 and superCritical_2 since the difference between these two series was the smallest both in entropy and algorithmic complexity. For testing this hypothesis we implemented following non-parametric bootstrap test of effect sizes invariance and mean. First, we resampled with replacement relevant groups 1,000 times (we did it for both normalized algorithmic complexity and normalized entropy). Second, for each resampling, we computed the effect size of the difference between critical_2 and superCritical_2 for algorithmic complexity and entropy. Third, we calculated $90\%$ confidence intervals for the tests of variance and mean.

```{r stat_fun}
stat_fun <- function(df = states_cmx, timestep = timestep, var_1 = "critical_2", var_2 = "superCritical_2", fun = mean) {
  df <- slice(df, !!timestep)
    stat_1 <- df %>% filter(name == var_2 & type == "compability") %$% fun(ncmx)
    stat_2 <- df %>% filter(name == var_1 & type == "compability") %$% fun(ncmx)
    stat_3 <- df %>% filter(name == var_2 & type == "compability") %$% fun(nent)
    stat_4 <- df %>% filter(name == var_1 & type == "compability") %$% fun(nent)
    (stat_1 - stat_2) - (stat_3 - stat_4)
}
```

```{r mean_test}
set.seed(8710)
## Critical_2 and superCritical_2
difference_mean <- boot(states_cmx, stat_fun, R = 1000, stype = "i", parallel = "multicore")
quantile(difference_mean$t[, 1], probs = c(.01, .99))
```

The $90\%\ CI[.0332\ .0381]$ showed that the effect size of the mean difference was bigger in algorithmic complexity than entropy. The algorithmic complexity showed a bigger difference in between critical_2 and superCiritical_2 than entropy.

```{r sd_test}
set.seed(8710)
## Critical_2 and superCritical_2
difference_var <- boot(states_cmx, fun = var, var_1 = "superCritical_2", var_2 = "critical_2", stat_fun, R = 1000, stype = "i", parallel = "multicore")
quantile(difference_var$t[, 1], probs = c(.01, .99))
```

The $90\%\ CI[.0014\ .0020]$ showed that the effect size of the variance difference was bigger in algorithmic complexity than entropy. The algorithmic complexity showed a bigger difference in variance between critical_2 and superCiritical_2 than entropy.
